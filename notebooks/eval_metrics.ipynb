{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae110ec9",
   "metadata": {},
   "source": [
    "# Évaluation StyleGAN2 (FairFace) — FID / IS / LPIPS\n",
    "\n",
    "Ce notebook :\n",
    "1) charge le générateur (EMA si présent) depuis `runs/fairface_cpu_lite/last.pt`  \n",
    "2) génère un échantillon synthétique  \n",
    "3) calcule **FID**, **Inception Score**, et **LPIPS diversité**  \n",
    "4) sauvegarde un grid d’images et un fichier `metrics.json`.\n",
    "\n",
    "> Remarque : si le checkpoint ne contient **pas** `mapper` et `label_emb`, on échantillonne\n",
    "directement **w ~ N(0,1)** (non conditionnel). Si `ckpt_*.pt` contient ces modules,\n",
    "ils seront utilisés pour un **échantillonnage conditionnel**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "799e16cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af86ae10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: lpips in c:\\users\\ilyes\\anaconda3\\lib\\site-packages (0.1.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ilyes\\anaconda3\\lib\\site-packages (4.66.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\ilyes\\anaconda3\\lib\\site-packages (10.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\ilyes\\anaconda3\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: numpy>1.20.0 in c:\\users\\ilyes\\anaconda3\\lib\\site-packages (from torchmetrics) (1.26.4)\n",
      "Requirement already satisfied: packaging>17.1 in c:\\users\\ilyes\\anaconda3\\lib\\site-packages (from torchmetrics) (23.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\ilyes\\anaconda3\\lib\\site-packages (from torchmetrics) (2.5.1)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
      "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: torchvision>=0.2.1 in c:\\users\\ilyes\\anaconda3\\lib\\site-packages (from lpips) (0.20.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ilyes\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ilyes\\anaconda3\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\ilyes\\anaconda3\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\ilyes\\anaconda3\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.13.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\ilyes\\anaconda3\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\ilyes\\anaconda3\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ilyes\\anaconda3\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ilyes\\anaconda3\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (2024.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ilyes\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ilyes\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0.0->torchmetrics) (2.1.3)\n",
      "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
      "   ---------------------------------------- 0.0/983.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 983.2/983.2 kB 23.3 MB/s  0:00:00\n",
      "Downloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
      "Installing collected packages: lightning-utilities, torchmetrics\n",
      "\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   ---------------------------------------- 2/2 [torchmetrics]\n",
      "\n",
      "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics lpips tqdm pillow scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24f37f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: C:\\Users\\ilyes\\Downloads\\stylegan2_cond\n",
      "facegan in path? True\n"
     ]
    }
   ],
   "source": [
    "# --- Réglages Windows/CPU pour éviter l’erreur OpenMP et limiter les threads ---\n",
    "import os, sys\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# --- RÉPERTOIRE RACINE DU PROJET : ADAPTE SI BESOIN ---\n",
    "ROOT = r\"C:\\Users\\ilyes\\Downloads\\stylegan2_cond\"\n",
    "\n",
    "# Se placer dans la racine du projet et l’ajouter au PYTHONPATH\n",
    "os.chdir(ROOT)\n",
    "if ROOT not in sys.path:\n",
    "    sys.path.insert(0, ROOT)\n",
    "\n",
    "print(\"cwd:\", os.getcwd())\n",
    "print(\"facegan in path?\", any(p.endswith(\"stylegan2_cond\") for p in sys.path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed123926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms as T\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from PIL import Image\n",
    "\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "import lpips  # LPIPS (réseau perceptuel)\n",
    "\n",
    "# Import des modules du projet (ça doit maintenant marcher)\n",
    "from facegan.models.generator import Generator, MappingNetwork\n",
    "from facegan.models.discriminator import ProjectionDiscriminator  # pas utilisé pour l'éval\n",
    "from facegan.data.dataset import FaceAttrsDataset  # pour récupérer les chemins réels\n",
    "\n",
    "ROOT   = r\"C:\\Users\\ilyes\\Downloads\\stylegan2_cond\"\n",
    "CSV    = os.path.join(ROOT, \"attrs.csv\")\n",
    "CKPT   = os.path.join(ROOT, \"runs\", \"fairface_cpu_lite\", \"last.pt\")   # ou un ckpt_*.pt si tu préfères\n",
    "OUTDIR = os.path.join(ROOT, \"runs\", \"fairface_cpu_lite\", \"eval\")\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")           # si tu as un GPU: torch.device(\"cuda\")\n",
    "SEED   = 123\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# Génération\n",
    "IMG_SIZE      = 256\n",
    "N_GEN         = 2000   # nb d'images synthétiques pour FID/IS (augmenter si tu peux)\n",
    "BATCH_GEN     = 16\n",
    "Z_DIM         = 128\n",
    "W_DIM         = 256\n",
    "LITE          = True   # = True pour correspondre à ton entraînement lite\n",
    "BASE_CH       = 32 if LITE else 64\n",
    "\n",
    "# LPIPS\n",
    "LPIPS_PAIRS   = 400    # nb de paires pour diversité (augmente si tu veux)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6270e12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 'real images used')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Charge dataset réel (pour FID) — on lit les images en 256x256 puis on convertira pour FID\n",
    "class RealImages(Dataset):\n",
    "    def __init__(self, csv, image_size=256):\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv(csv)\n",
    "        self.paths = df[\"path\"].tolist()\n",
    "        self.t = T.Compose([\n",
    "            T.Resize((image_size, image_size), interpolation=T.InterpolationMode.BILINEAR),\n",
    "            T.ToTensor(),                    # [0,1]\n",
    "        ])\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, i):\n",
    "        img = Image.open(self.paths[i]).convert(\"RGB\")\n",
    "        return self.t(img)\n",
    "\n",
    "real_ds = RealImages(CSV, IMG_SIZE)\n",
    "# On ne prend que N_GEN images réelles pour rendre la comparaison équitable\n",
    "subset_idx = np.random.choice(len(real_ds), size=min(N_GEN, len(real_ds)), replace=False)\n",
    "real_loader = DataLoader(torch.utils.data.Subset(real_ds, subset_idx),\n",
    "                         batch_size=BATCH_GEN, shuffle=False, num_workers=0)\n",
    "len(real_loader.dataset), \"real images used\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff5e6423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint keys chargés: ['emaG']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ilyes\\AppData\\Local\\Temp\\ipykernel_30956\\3037672785.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(CKPT, map_location=DEVICE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (blocks): ModuleList(\n",
       "    (0-2): 3 x StyledConv(\n",
       "      (conv): ModulatedConv2d(\n",
       "        (affine): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (3-4): 2 x StyledConv(\n",
       "      (conv): ModulatedConv2d(\n",
       "        (affine): Linear(in_features=256, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (5-6): 2 x StyledConv(\n",
       "      (conv): ModulatedConv2d(\n",
       "        (affine): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (7-13): 7 x StyledConv(\n",
       "      (conv): ModulatedConv2d(\n",
       "        (affine): Linear(in_features=256, out_features=32, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (torgb): ToRGB(\n",
       "    (conv): ModulatedConv2d(\n",
       "      (affine): Linear(in_features=256, out_features=32, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Construit le générateur et charge le checkpoint\n",
    "#    Si 'emaG' est présent -> on l'utilise; sinon 'G'.\n",
    "#    Si le ckpt contient 'mapper'/'label_emb', on utilisera le sampling conditionnel.\n",
    "\n",
    "# Embedding dims utilisés dans le train lite plus haut\n",
    "D_AGE, D_GEN, D_ETH = 16, 8, 16\n",
    "\n",
    "G = Generator(w_dim=W_DIM, base_ch=BASE_CH).to(DEVICE)\n",
    "mapper = MappingNetwork(z_dim=Z_DIM, c_dim=D_AGE + D_GEN + D_ETH, w_dim=W_DIM,\n",
    "                        n_layers=4 if LITE else 8).to(DEVICE)\n",
    "\n",
    "# Labels embeddings (si on peut charger depuis ckpt)\n",
    "label_emb = torch.nn.ModuleDict({\n",
    "    \"age\": torch.nn.Embedding(5, D_AGE),\n",
    "    \"gen\": torch.nn.Embedding(2, D_GEN),\n",
    "    \"eth\": torch.nn.Embedding(7, D_ETH),\n",
    "}).to(DEVICE)\n",
    "\n",
    "ckpt = torch.load(CKPT, map_location=DEVICE)\n",
    "loaded = []\n",
    "\n",
    "if \"emaG\" in ckpt:\n",
    "    G.load_state_dict(ckpt[\"emaG\"]); loaded.append(\"emaG\")\n",
    "elif \"G\" in ckpt:\n",
    "    G.load_state_dict(ckpt[\"G\"]); loaded.append(\"G\")\n",
    "\n",
    "if \"mapper\" in ckpt:\n",
    "    mapper.load_state_dict(ckpt[\"mapper\"]); loaded.append(\"mapper\")\n",
    "else:\n",
    "    mapper = None\n",
    "\n",
    "if \"label_emb\" in ckpt:\n",
    "    label_emb.load_state_dict(ckpt[\"label_emb\"]); loaded.append(\"label_emb\")\n",
    "else:\n",
    "    label_emb = None\n",
    "\n",
    "print(\"Checkpoint keys chargés:\", loaded)\n",
    "G.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "350e3f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ilyes\\\\Downloads\\\\stylegan2_cond\\\\runs\\\\fairface_cpu_lite\\\\eval\\\\samples_grid.png'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4) Utilitaires\n",
    "\n",
    "def to_uint8(img):\n",
    "    \"\"\"\n",
    "    img: tensor BxCxHxW dans [-1,1] ou [0,1].\n",
    "    Retourne uint8 [0..255] (B,C,H,W).\n",
    "    \"\"\"\n",
    "    if img.min() < 0:\n",
    "        x = (img.clamp(-1,1) + 1) * 0.5\n",
    "    else:\n",
    "        x = img.clamp(0,1)\n",
    "    return (x * 255).round().to(torch.uint8)\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_fake(batch, device, conditional=True):\n",
    "    \"\"\" Génère batch images soit conditionnelles (si mapper+embeds connus), sinon w~N(0,1). \"\"\"\n",
    "    if conditional and (mapper is not None) and (label_emb is not None):\n",
    "        # échantillonner labels uniformes (ou selon CSV si tu préfères)\n",
    "        age = torch.randint(0,5,(batch,), device=device)\n",
    "        gen = torch.randint(0,2,(batch,), device=device)\n",
    "        eth = torch.randint(0,7,(batch,), device=device)\n",
    "        z = torch.randn(batch, Z_DIM, device=device)\n",
    "        c = torch.cat([label_emb[\"age\"](age), label_emb[\"gen\"](gen), label_emb[\"eth\"](eth)], dim=1)\n",
    "        w = mapper(z, c)\n",
    "        x = G(w)\n",
    "    else:\n",
    "        # non conditionnel: w ~ N(0,1)\n",
    "        w = torch.randn(batch, W_DIM, device=device)\n",
    "        x = G(w)\n",
    "    return x\n",
    "\n",
    "# Sauvegarde un petit grid d'images générées pour sanity check\n",
    "with torch.no_grad():\n",
    "    sample = sample_fake(32, DEVICE, conditional=True)\n",
    "grid = make_grid((sample.clamp(-1,1)+1)/2, nrow=8)\n",
    "save_image(grid, os.path.join(OUTDIR, \"samples_grid.png\"))\n",
    "os.path.join(OUTDIR, \"samples_grid.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbb53128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/toshas/torch-fidelity/releases/download/v0.2.0/weights-inception-2015-12-05-6726825d.pth\" to C:\\Users\\ilyes/.cache\\torch\\hub\\checkpoints\\weights-inception-2015-12-05-6726825d.pth\n",
      "100%|██████████| 91.2M/91.2M [00:01<00:00, 66.3MB/s]\n",
      "c:\\Users\\ilyes\\anaconda3\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "FID: real: 100%|██████████| 125/125 [06:56<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID: 265.540 | IS: 2.682 ± 0.173\n"
     ]
    }
   ],
   "source": [
    "# 5) FID & IS\n",
    "fid = FrechetInceptionDistance(feature=2048)   # torchmetrics gère le resize interne\n",
    "isc = InceptionScore(splits=10, normalize=True)\n",
    "\n",
    "# (A) accumulate REAL\n",
    "for real in tqdm(real_loader, desc=\"FID: real\"):\n",
    "    # real: [B,3,H,W] en [0,1]\n",
    "    fid.update((real*255).to(torch.uint8), real=True)\n",
    "\n",
    "# (B) accumulate FAKE\n",
    "n_done = 0\n",
    "while n_done < N_GEN:\n",
    "    b = min(BATCH_GEN, N_GEN - n_done)\n",
    "    with torch.no_grad():\n",
    "        fake = sample_fake(b, DEVICE, conditional=True)    # [-1,1]\n",
    "    # FID attend uint8 0..255\n",
    "    fid.update(to_uint8(fake).cpu(), real=False)\n",
    "\n",
    "    # IS attend float [0,1]\n",
    "    isc.update(((fake.clamp(-1,1)+1)/2).cpu())\n",
    "\n",
    "    n_done += b\n",
    "\n",
    "fid_score = fid.compute().item()\n",
    "is_mean, is_std = isc.compute()\n",
    "is_mean, is_std = float(is_mean), float(is_std)\n",
    "print(f\"FID: {fid_score:.3f} | IS: {is_mean:.3f} ± {is_std:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2673cd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\ilyes\\anaconda3\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "LPIPS (diversité) mean=0.4186 ± 0.1295\n"
     ]
    }
   ],
   "source": [
    "# 6) LPIPS diversité (moyenne sur paires synthétiques)\n",
    "loss_fn = lpips.LPIPS(net='alex')  # plus léger\n",
    "loss_fn = loss_fn.to(DEVICE).eval()\n",
    "\n",
    "pairs = 0\n",
    "lpips_vals = []\n",
    "with torch.no_grad():\n",
    "    while pairs < LPIPS_PAIRS:\n",
    "        b = 2   # on génère 2 images et on calcule 1 distance\n",
    "        imgs = sample_fake(b, DEVICE, conditional=True)  # [-1,1]\n",
    "        d = loss_fn(imgs[0:1], imgs[1:2]).item()\n",
    "        lpips_vals.append(d)\n",
    "        pairs += 1\n",
    "\n",
    "lpips_mean = float(np.mean(lpips_vals))\n",
    "lpips_std  = float(np.std(lpips_vals))\n",
    "print(f\"LPIPS (diversité) mean={lpips_mean:.4f} ± {lpips_std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8308894f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
